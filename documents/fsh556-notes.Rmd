---
title: "fsh556 Notes"
author: "Dan Ovando"
date: "3/27/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = FALSE, message = FALSE, warning = FALSE)
library(TMB)
library(tidyverse)


```

# lecture 1

second derivative informs whether a minimum is a "true" global minimum, first derivative for local minima. 

## why maximum likelihood ?

You can transform parameters and still end up with the same likelihood profile, which is not necessarily true of Bayesian models

consistent in the limit

asymptotic normality


# lab 1

poisson, number of people that hit a website in a given time

decay of radioactive material 

newton was the first optimizer! quasi newton methods, optimize by second derivatives

nelder-mead is like a little aomeba that searches around with a different leg for each parameter and just hunts aroung. With TMB, you pass a gradient to nelder-mead

check that hessian matrix is positive definite



# lecture-2

we like consistency (will converge to the best estimate of the correct model or the best for the mispecified model) and asymptotic normality 

why is stein's paradox a paradox? you'd expecte the mean for each person to be the best predictor for that person, rather than a better predictor is that person with some shrinkage towards the mean

w in shrinkage slides is precision, or inverse variance

residual variance in lme4 is the variance of the error term in a linear model

A full rank matrix every eigen value of the matrix is greater than zero. If it is rank deficient then one or more eigen values will be basically zero. We need full rank for laplace approximation

Rank of a matrix is the number of linearaly independent columns in a matrix

Lapalce approximation fails if one or more matrices is rank deficient. 

Standard errors will also fail if one matrix is rank deficient 

If you estimate the model and that works but the standard errors fail, then its fixed effects problem, if it fails in the fitting stage, the problem may be in the random effect

Why treat "study" as a random effect? Might have very little data for any one individual study, so random effect might help. Hierarchichal model gives you what you might actually want which is the mean effect across the the effects. 

TO invert a matrix you need it to be full rank. So, if the hessian is no rank, then you can't invert it, so you can't get the standard errors


# lecture 3 - kalman filters

one state space definition - the latent variable has a time series process, with some markovian process

consistency in time series models. two forms of consistency in time series. If you add more data within your model period, infill asymptotics, and if you add more time periods but the same number of data points per time periods thats sprawl asymptotics. There are two different implications of these two things

REML - restricted maximum likelihood

there's no guarantee that maximum likelihood is unbiased for a finite sample size

e.g. variance. If you look at th maximum likelihood estimate of the variance for a linear model, it will just be the average squared deviation. Gets shrunk to the mean of the data and not the "true" value. 

use REML to treat non-variance fixed effects as random, not entirely sure why


# lab 3

gompertz model. Growth rate goes to infinity at the extreme

alpha over beta is the carrying capacity

look up stochastic SRA

autoregreesive term, it's the simplist biological model that is identical to a linear model

sigma here is a process error

when rho is negative in an autoregressive model you teh osscilation

map function in TMB

the mpa input has two basic roles, one is turning off a parameter, and the other is mirroring a parameter, or making some set of parameters in in the model all equal. 

e.g. comparing a mixed effects model to a pure linear model. 

map in this way basically fixes things at starting values, it's not actually setting things to NA?

NA turns off, a level sets a initial value?

reminder on inner and outer optimizer

inner is the lapace approximation solving the random effects holding the fixed effects constant. Given the value of fixed effects, optimize random effects, do lapace approximation. 

Then, the outer loop is holding the random effects constant, and then optimizing the fixed effects

in the pollock example, sigma b is the process error

of process and observation error are independent, then variances should be additivie

standard error is just the standard deviation between your samples n divided by the square root of n if all n are independent

the claim is that we can estimate the standard error around the mean from the variance of the sampling. 

Need to calculate the standard error for the index and convert to the standard error of a log-normal file

Field sampling variance as a lower bound 

lab 1

```{r}
Use_REML = FALSE
set.seed(2)

#####################
# Explore Gompertz model
#####################

beta= 0.2
alpha = 1
d_equil = exp(alpha/beta)

d_1 = seq(0,d_equil*2,length=1e4)
d_2 = d_1 * exp(alpha) * exp( - beta*log(d_1))

# Dynamics
  par( mfrow=c(1,2), mar=c(3,3.5,2,0), mgp=c(1.75,0.25,0), tck=-0.02)
  plot( x=d_1, y=d_2, type="l", lwd=3, xlab=expression(Biomass[t]), ylab=expression(Biomass[t+1]), main="Production")
  abline( a=0, b=1, lty="dotted")
  # Log-dynamics
  plot( x=log(d_1[-1]), y=log(d_2[-1]/d_1[-1]), type="l", lwd=3, xlab=expression(log(Biomass[t])), ylab=expression(log(Biomass[t+1]/Biomass[t])), main="log-Biomass ratio" )
  abline( a=1, b=0, lty="dotted")

######################
# Simulate data
######################

nt = 100
log_d0 = 3
sigmaP = 0.5
sigmaM = 0.5
alpha = 1
beta = 0.1

# Simulate predictors
log_d_t = log_b_t = rep(NA, nt)
log_d_t[1] = log_d0
for( t in 2:nt ){
  log_d_t[t] = alpha + (1-beta)*log_d_t[t-1] + rnorm( 1, mean=0, sd=sigmaP )
}
for( t in 1:nt ){
  log_b_t[t] = log_d_t[t] + rnorm( 1, mean=0, sd=sigmaM )
}



######################
# Run in TMB
######################

library(TMB)

# Compile model
Version = "gompertz"
compile( paste0(Version,".cpp") )

# Build inputs
Data = list( "nt"=nt, "log_b_t"=log_b_t )
Parameters = list( "log_d0"=0, "log_sigmaP"=1, "log_sigmaM"=1, "alpha"=0, "rho"=0, "log_d_t"=rep(0,Data$nt) )
Random = c("log_d_t")
if( Use_REML==TRUE ) Random = union( Random, c("log_d0","alpha","rho") )

# Build object
dyn.load( dynlib("gompertz") )
Obj = MakeADFun(data=Data, parameters=Parameters, random=Random, DLL="gompertz")  #

# Prove that function and gradient calls work
Obj$fn( Obj$par )
Obj$gr( Obj$par )

# Optimize
start_time = Sys.time()
Opt = nlminb( start=Obj$par, objective=Obj$fn, gradient=Obj$gr, control=list("trace"=1) )
  Opt[["final_gradient"]] = Obj$gr( Opt$par )
  Opt[["total_time"]] = Sys.time() - start_time

# Get reporting and SEs
Report = Obj$report()
  Opt$SD = sdreport( Obj )

for( z in 1:3 ){
  png( file=paste0("gompertz_",z,".png"), width=8, height=5, res=200, units="in" )
    par( mar=c(3,3,1,1), mgp=c(2,0.5,0), tck=-0.02 )
    plot( x=1:Data$nt, y=Data$log_b_t, col="blue", cex=1.2, xlab="Time", ylab="Value", pch=20 )
    if(z>=2){
      points( x=1:Data$nt, y=as.list(Opt$SD,"Estimate")$log_d_t, col="red" )
      for( t in 1:Data$nt) lines( x=rep(t,2), y=as.list(Opt$SD,"Estimate")$log_d_t[t]+c(-1.96,1.96)*as.list(Opt$SD,"Std. Error")$log_d_t[t], col="red" )
    }
    if(z>=3) lines( x=1:Data$nt, y=log_d_t, col="black", lwd=2 )
  dev.off()
}

######################
# Download real data and run again
######################

# devtools::install_github("james-thorson/FishData")

# Download data for Alaska pollock
CPUE = FishData::download_catch_rates( survey="Eastern_Bering_Sea", species_set="Gadus chalcogrammus", error_tol=0.01, localdir=paste0(getwd(),"/") )
B_t = tapply( CPUE[,'Wt'], INDEX=CPUE[,'Year'], FUN=mean )

sd_bt <- CPUE %>% 
  as_data_frame() %>% 
  group_by(Year) %>% 
  summarise(sd_b = sd(Wt),
            mean_b = mean(Wt),
            n = length(Wt)) %>% 
  ungroup() %>% 
  mutate(se_sampling = map2_dbl(sd_b,n, ~.x /sqrt(.y))) %>% 
  mutate(cv_sampling = se_sampling  / mean_b,
         log_sigma_sampling = log(cv_sampling^2 + 1))

# calculate sampling variance and standard errors

# Run Gompertz model again
Data = list( "nt"=length(B_t), "log_b_t"=log(B_t) )
Parameters = list( "log_d0"=0, "log_sigmaP"=1, "log_sigmaM"=1, "alpha"=0, "rho"=0, "log_d_t"=rep(0,Data$nt), log_sigma_sampling = log(mean(sd_bt$log_sigma_sampling)))

compile(here::here("src","gompertz_lab.cpp"))
dyn.load( dynlib(here::here("src","gompertz_lab")) )

Random = c("log_d_t")

Obj = MakeADFun(data=Data, parameters=Parameters, random=Random, DLL="gompertz_lab")  #
Opt = TMBhelper::Optimize( obj=Obj, lower = list(log_sigma_sampling =log(mean(sd_bt$log_sigma_sampling))) )

# Get reporting and SEs
Report = Obj$report()

for( z in 1:2 ){
  # png( file=paste0("pollock_",z,".png"), width=8, height=5, res=200, units="in" )
    par( mar=c(3,3,1,1), mgp=c(2,0.5,0), tck=-0.02 )
    plot( x=1:Data$nt, y=Data$log_b_t, col="blue", cex=1.2, xlab="Time", ylab="Value", pch=20 )
    if(z>=2){
      points( x=1:Data$nt, y=as.list(Opt$SD,"Estimate")$log_d_t, col="red" )
      for( t in 1:Data$nt) lines( x=rep(t,2), y=as.list(Opt$SD,"Estimate")$log_d_t[t]+c(-1.96,1.96)*as.list(Opt$SD,"Std. Error")$log_d_t[t], col="red" )
    }
  # dev.off()
}

write.csv( cbind(Opt$diagnostics, summary(Opt$SD,"fixed")[,'Std. Error']), file=paste0(getwd(),"/pollock.csv") )


```

